---
title: Switch Overview
sidebar_position: 4
---

import CodeBlock from '@theme/CodeBlock';

**Switch** is a Lakebridge transpiler plugin that uses Large Language Models (LLMs) to convert SQL code into Databricks notebooks. Switch leverages [Mosaic AI Model Serving](https://docs.databricks.com/aws/en/machine-learning/model-serving/) to understand SQL intent and semantics, generating equivalent Python notebooks with Spark SQL.

This LLM-powered approach excels at converting complex stored procedures, business logic, and ETL workflows where context and intent matter more than syntactic transformation. While generated notebooks may require manual adjustments, they provide a valuable foundation for Databricks migration.

---

## How Switch Works

Switch operates through three key components that distinguish it from rule-based transpilers:

### LLM-Powered Semantic Understanding
Instead of parsing rules, Switch uses [Mosaic AI Model Serving](https://docs.databricks.com/aws/en/machine-learning/model-serving/) to:
- Interpret SQL intent and business context beyond syntax
- Handle proprietary SQL extensions and complex logic patterns
- Support extensible conversion through custom YAML prompts

### Native Databricks Integration
Switch runs entirely within the Databricks platform:
- **Jobs API**: Executes as scalable Databricks Jobs for batch processing
- **Delta Tables**: Tracks conversion progress with restart capabilities  
- **Model Serving**: Direct integration with Databricks LLM endpoints
- **Parallel Processing**: Leverages cluster resources for multiple files

### Flexible Output Formats
- **Primary Output**: Python notebooks containing Spark SQL
- **Experimental**: SQL notebook conversion for pure SQL workflows

---

## Prerequisites & Requirements

Before installing and using Switch, ensure your Databricks environment meets these requirements:

### Databricks Workspace Setup

**Job Execution Environment**
- **Job Creation Permission**: Ability to create and manage Databricks Jobs
  - A Switch job is automatically created during `install-transpile` execution
- **Compute Access**: Switch creates jobs using serverless job compute by default
  - If serverless is unavailable, you can modify job configuration in the Databricks workspace to use classic job compute

**Data Storage Requirements**
- **Catalog Access**: Existing catalog for Switch state management tables
  - Unity Catalog catalogs (recommended) or Hive metastore catalogs supported
  - Default catalog name: `remorph`
- **Schema Access**: Existing schema within the specified catalog
  - Default schema name: `transpiler`
- **Table Creation Permission**: `CREATE TABLE` rights on the catalog and schema
  - Switch creates Delta tables for conversion state and intermediate results

### Model Serving Access

**LLM Endpoint Requirements**
- **Foundation Model API**: Default endpoint is `databricks-claude-sonnet-4`
- **Custom Endpoints**: You can configure custom or external model serving endpoints
- **Token Quota**: Sufficient quota for your conversion workload
- **Permissions**: Query access to the configured model serving endpoint

### Databricks Runtime (for Classic Job Compute)
- **Default**: Serverless job compute (no runtime configuration required)
- **Classic Job Compute**: If using classic clusters instead of serverless:
  - **Verified Versions**: DBR 14.3 LTS or 15.3 LTS (higher versions likely compatible)
  - **Compute Type**: Single-node clusters recommended (Photon not required)

---

## SQL Dialect Support

Switch provides built-in sample conversion prompts for 8 major SQL dialects, with extensibility for custom dialects through YAML configuration.

### Built-in Sample Conversion Prompts

| SQL Dialect | Source Systems |
|-------------|----------------|
| **MySQL** | MySQL, MariaDB, and MySQL-compatible services (including Amazon Aurora MySQL, RDS, Google Cloud SQL) |
| **Netezza** | IBM Netezza |
| **Oracle** | Oracle Database, Oracle Exadata, and Oracle-compatible services (including Amazon RDS) |
| **PostgreSQL** | PostgreSQL and PostgreSQL-compatible services (including Amazon Aurora PostgreSQL, RDS, Google Cloud SQL) |
| **Redshift** | Amazon Redshift |
| **Snowflake** | Snowflake |
| **Teradata** | Teradata |
| **T-SQL** | Microsoft SQL Server, Azure SQL family (Database, Managed Instance, Synapse Analytics), Amazon RDS for SQL Server |

### Custom Dialect Support

Switch's LLM-based architecture supports additional SQL dialects through custom YAML conversion prompts, making it extensible beyond built-in options.

For custom prompt creation, see the [Advanced Usage](#advanced-usage) section.

---

## Installation & Usage

### Installation

Switch integrates with the Lakebridge transpiler ecosystem:

```bash
databricks labs lakebridge install-transpile
```

The installation automatically:

1. **Installs Switch**: Alongside other transpilers
2. **Creates Databricks Job**: In your authenticated workspace  
3. **Uploads Notebooks**: Switch processing notebooks to workspace

Configuration is saved to:
- **macOS/Linux**: `~/.databricks/labs/remorph-transpilers/switch/lib/config.yml`
- **Windows**: `%USERPROFILE%\.databricks\labs\remorph-transpilers\switch\lib\config.yml`

### CLI Usage

Use the following command to run Switch. Specifying the Switch config file with `--transpiler-config-path` tells Lakebridge to use Switch as the transpiler:

```bash
# Switch usage
databricks labs lakebridge transpile \
  --transpiler-config-path ~/.databricks/labs/remorph-transpilers/switch/lib/config.yml \
  --input-source /Workspace/path/to/sql \
  --output-folder /Workspace/path/to/notebooks \
  --source-dialect snowflake \
  --catalog-name your_existing_catalog \
  --schema-name your_existing_schema
```

For advanced configuration options, see the [Advanced Configuration](#advanced-configuration) section below.

#### Operational Notes

Switch operates differently from other Lakebridge transpilers:

- **Databricks Workspace Paths Required**: Input and output paths must be workspace paths (e.g., `/Workspace/path/to/...`) rather than local file paths
- **Jobs API Execution**: Switch runs as a Databricks Job in your workspace, not as a local process
- **Asynchronous by Default**: The command returns immediately with a job URL, allowing you to monitor progress in the Databricks workspace
- **Monitoring**: Use the returned job URL to track conversion progress and view logs

### CLI Parameters

#### Required Parameters
- `--transpiler-config-path` - Path to Switch configuration file (identifies Switch as the transpiler)
- `--input-source` - Path(s) containing SQL files to convert (see Input Source Patterns below)
- `--output-folder` - Workspace path for generated notebooks
- `--source-dialect` - SQL dialect: `mysql`, `netezza`, `oracle`, `postgresql`, `redshift`, `snowflake`, `teradata`, `tsql`
- `--catalog-name` - Databricks catalog for state tables (default: `remorph`). Must already exist.
- `--schema-name` - Databricks schema for state tables (default: `transpiler`). Must already exist.

#### Input Source Patterns

The `--input-source` parameter supports flexible path specifications:

| Pattern Type | Example | Description |
|-------------|---------|-------------|
| **Single Directory** | `/Workspace/path/to/sql` | Process all files in specified directory recursively |
| **Multiple Directories** | `/Workspace/project1/sql,/Workspace/project2/sql` | Process files from multiple directories recursively (comma-separated) |
| **Glob Patterns** | `/Workspace/Users/*/sql/*.sql` | Use wildcards to match paths (`*` = any characters, `**` = recursive) |
| **Mixed Patterns** | `/Workspace/prod/**/*.sql,/Workspace/dev/sql` | Combine different pattern types |

#### Permission Requirements

The user must have `CREATE TABLE` permissions on the specified catalog and schema for Switch's state management and intermediate result tables.

#### Advanced Configuration

Switch provides detailed configuration options that can be set during installation or by editing the `config.yml` file directly:

| Parameter | Description | Default Value | Available Options |
|-----------|-------------|---------------|-------------------|
| `endpoint_name` | Model serving endpoint name | `databricks-claude-sonnet-4` | Any valid endpoint name |
| `token_count_threshold` | Maximum tokens per file for processing | `20000` | Any positive integer |
| `concurrency` | Number of parallel LLM requests | `4` | Any positive integer |
| `comment_lang` | Language for generated comments | `English` | `English`, `Japanese`, `Chinese`, `Spanish`, `French`, `German` |
| `max_fix_attempts` | Maximum syntax error fix attempts | `1` | Any positive integer |
| `log_level` | Logging verbosity level | `INFO` | `DEBUG`, `INFO`, `WARNING`, `ERROR` |
| `conversion_prompt_yaml` | Custom conversion prompt YAML file path | `<none>` | Full workspace path to YAML file |
| `existing_result_table` | Existing result table to reuse | `<none>` | Full table name (`catalog.schema.table`) |
| `sql_output_dir` | (Experimental) Directory for SQL notebooks | `<none>` | Full workspace path |
| `wait_for_completion` | Wait for job completion (synchronous mode) | `false` | `true`, `false` |

**Configuration Methods**:
1. **During Installation**: Configure options when running `databricks labs lakebridge install-transpile`
2. **Direct File Edit**: Modify the `config.yml` file at the installation path and then execute `databricks labs lakebridge transpile`

### Job Monitoring

After executing the transpile command, Switch runs **asynchronously by default** and returns job execution details in JSON format:

```json
{
  "transpiler": "switch",
  "job_id": 12345,
  "run_id": 67890,
  "run_url": "https://your-workspace.databricks.com/#job/12345/run/67890"
}
```

Access the `run_url` to monitor job progress in your Databricks workspace.

**Job Execution Modes**:
- **Asynchronous (default)**: Command returns immediately with monitoring URL
- **Synchronous**: Set `wait_for_completion: true` in transpiler config to wait for job completion

---

## Use Cases

### When to Use Switch

Switch's LLM-based approach makes it particularly suitable for:
- **Complex stored procedures and ETL logic** where understanding business intent is crucial
- **SQL dialects not yet covered by other transpilers** — Switch allows you to add support through custom YAML prompts

### When to Use Other Transpilers

Morpheus and BladeBridge are particularly suitable for:
- **Interactive conversion** with immediate processing
- **Deterministic output requirements** with guaranteed syntax equivalence
- **High-volume processing** with faster throughput than Switch

Choose the approach based on your project's complexity, accuracy requirements, and processing time constraints.

### Success Patterns

#### Effective Switch Usage:
1. **Start Small**: Begin with representative sample files
2. **Iterate Prompts**: Refine YAML prompts based on initial results  
3. **Review and Refine**: Treat output as a solid foundation requiring refinement
4. **Document Patterns**: Capture successful prompt patterns for reuse

---

## Databricks Implementation Details

When you run Switch via the CLI, it executes as Databricks Jobs using a sophisticated multi-stage processing pipeline. This section covers the internal architecture and configuration options.

### Processing Architecture

Switch executes as a Databricks Job that runs the main orchestration notebook (`00_main`), which coordinates a 6-stage conversion pipeline:

#### Main Orchestration
The **`00_main`** notebook serves as the entry point when Switch is executed via Databricks Jobs API. It:
- Validates all input parameters from the job configuration
- Executes each processing stage in sequence using `dbutils.notebook.run()`
- Tracks overall progress and handles stage-level failures
- Returns the final conversion status to the job runner

#### Conversion Flow

```mermaid
flowchart TD
    input[Input SQL Files] -->|Input| analyze[[01_analyze_input_files]]
    analyze <-->|Read & Write| conversionTable[Conversion Result Table]

    conversionTable <-->|Read & Write| convert[[02_convert_sql_to_databricks]]
    convert -.->|Use| endpoint[Model Serving Endpoint]
    convert -.->|Refer| prompts["Conversion Prompt YAML<br/>(SQL Dialect Specific)"]

    conversionTable <-->|Read & Write| validate[[03_01_static_syntax_check]]

    conversionTable <-->|Read & Write| fixErrors[[03_02_fix_syntax_error]]
    fixErrors -.->|Use| endpoint

    conversionTable <-->|Read & Write| splitCells[[04_split_cells]]

    conversionTable -->|Input| export[[05_export_to_databricks_notebooks]]
    export -->|Output| notebooks[Converted Databricks Notebooks]

    %% Styling
    classDef process fill:#E6E6FA,stroke:#333,stroke-width:2px;
    class analyze,convert,validate,fixErrors,splitCells,export process;
    classDef data fill:#E0F0E0,stroke:#333,stroke-width:2px;
    class input,conversionTable,notebooks data;
    classDef external fill:#FFF0DB,stroke:#333,stroke-width:2px;
    class endpoint,prompts external;
```

The conversion pipeline consists of 6 core stages plus 1 optional stage, each implemented as a separate Databricks notebook. The following sections provide detailed explanations of each processing stage shown in the flow above.

##### Stage 1: File Analysis
The pipeline begins by scanning your input directory for SQL files. It removes SQL comments to get accurate token counts using model-specific tokenizers (Claude uses ~3.4 characters per token, OpenAI uses tiktoken). Files exceeding the `token_count_threshold` are excluded from conversion. All metadata is stored in a timestamped Delta table for tracking.

##### Stage 2: SQL Conversion
The system loads dialect-specific YAML prompts (or your custom prompt) and sends SQL content to the configured model serving endpoint. Multiple files are processed concurrently (default: 4) for efficiency. The LLM transforms SQL code into Python code with `spark.sql()` calls, preserving business logic while adapting to Databricks patterns.

##### Stage 3: Syntax Validation
Before proceeding, all generated code undergoes rigorous validation. Python syntax is checked using `ast.parse()`, while SQL statements within `spark.sql()` calls are validated using Spark's `EXPLAIN` command. Any errors are recorded in the result table without modifying the code.

##### Stage 4: Error Correction
If syntax errors are detected, this stage attempts automatic fixes. It sends the error context back to the LLM, which suggests corrections. The process repeats up to `max_fix_attempts` times (default: 1). This stage only runs when errors exist from the previous validation.

##### Stage 5: Cell Organization
Raw converted code is transformed into a well-structured notebook. The system analyzes code flow and dependencies, splitting at logical boundaries like imports, function definitions, and major SQL operations. Markdown cells are added for documentation and readability.

##### Stage 6: Notebook Export
The final stage creates Databricks-compatible `.py` notebooks in your output directory. Each notebook includes metadata, source file references, and any syntax check results as comments. The system handles files up to 10MB and preserves your directory structure.

##### Stage 7: SQL Notebook Conversion (Optional)
When `sql_output_dir` is specified, this experimental stage uses the model serving endpoint to convert Python notebooks into SQL notebook format with Databricks SQL syntax. This is useful for teams preferring SQL-only workflows, though some Python logic may be lost in translation.

### Parameter Mapping

When Switch executes as a Databricks Job, CLI parameters are mapped to notebook parameters:

| CLI Parameter | Notebook Parameter | Description |
|---------------|-------------------|-------------|
| `--input-source` | `input_dir` | Path(s) containing SQL files (supports patterns) |
| `--output-folder` | `output_dir` | Directory for generated notebooks |
| `--catalog-name` | `result_catalog` | Catalog for tracking tables |
| `--schema-name` | `result_schema` | Schema for tracking tables |
| `--source-dialect` | `sql_dialect` | Input SQL dialect |
| `--transpiler-config-path` | (config file) | Loads additional parameters from Switch config |

Additional parameters are configured through the Switch config file (see the [Advanced Configuration](#advanced-configuration) section above).

### State Management

Switch uses a Delta table to track conversion progress and results. Each conversion job creates a timestamped table: `{catalog}.{schema}.lakebridge_switch_{YYYYMMDDHHmm}`

The table stores input file information (path, content, token counts), conversion results (generated notebooks, token usage, processing time), error details when conversions fail, and syntax check results from validation stages. This allows you to monitor which files were processed successfully and investigate any issues that occurred during conversion.

#### Conversion Result Table Schema

Switch creates Delta tables with the following complete schema:

| Column | Type | Description |
|--------|------|-------------|
| `input_file_number` | int | Unique integer identifier for each input file (starts from 1) |
| `input_file_path` | string | Full path to the input file |
| `input_file_encoding` | string | Detected encoding of the input file (e.g., UTF-8) |
| `tokenizer_type` | string | Type of tokenizer used (claude or openai) |
| `tokenizer_model` | string | Specific tokenizer model/encoding used |
| `input_file_token_count` | int | Total number of tokens in the input file |
| `input_file_token_count_without_sql_comments` | int | Token count excluding SQL comments |
| `input_file_content` | string | Entire content of the input file |
| `input_file_content_without_sql_comments` | string | Content excluding SQL comments |
| `is_conversion_target` | boolean | Whether file should be processed (updated during conversion) |
| `model_serving_endpoint_for_conversion` | string | Model endpoint used for conversion |
| `model_serving_endpoint_for_fix` | string | Model endpoint used for syntax error fixing |
| `request_params_for_conversion` | string | Conversion request parameters in JSON format |
| `request_params_for_fix` | string | Fix request parameters in JSON format |
| `result_content` | string | Generated notebook content (initially null) |
| `result_prompt_tokens` | int | Number of prompt tokens used (initially null) |
| `result_completion_tokens` | int | Number of completion tokens generated (initially null) |
| `result_total_tokens` | int | Total tokens used (prompt + completion, initially null) |
| `result_processing_time_seconds` | float | Processing time in seconds (initially null) |
| `result_timestamp` | timestamp | UTC timestamp when processing completed (initially null) |
| `result_error` | string | Any conversion errors encountered (initially null) |
| `result_python_parse_error` | string | Python syntax errors found using ast.parse |
| `result_extracted_sqls` | array<string> | SQL statements extracted from Python code (initially null) |
| `result_sql_parse_errors` | array<string> | SQL syntax errors found using EXPLAIN (initially null) |

### Model Requirements

#### Supported LLM Endpoints

Switch works with LLMs that have large context windows and strong code comprehension capabilities. Databricks Foundation Model APIs provide the simplest setup, while external models offer additional flexibility for organizational requirements.

**Primary Recommendation**:
Currently, the latest Claude models available through Foundation Model APIs show the best performance for understanding complex code patterns and business logic in SQL-to-notebook conversion.

For complex stored procedures and intricate business logic, Claude's extended thinking mode can significantly improve conversion accuracy. This mode allows the model to reason through complex transformations more thoroughly, though it increases processing time and token usage. To enable extended thinking mode, configure the `request_params` parameter (example values):
```json
{"max_tokens": 64000, "thinking": {"type": "enabled", "budget_tokens": 16000}}
```

Other capable models are also supported through both Foundation Model APIs and external models.

**External Models**:
For organizations with specific requirements (such as Azure-only policies, custom compliance needs, or high-throughput processing), Switch supports external models through external model serving endpoints. External models offer benefits including:
- **Throughput Control**: Configure dedicated capacity for consistent performance
- **Organizational Compliance**: Meet specific cloud or vendor requirements
- **Cost Management**: Optimize costs for large-scale processing
- **Higher Concurrency**: Scale beyond Foundation Model API limitations

**Choosing Between Options**:
- **Foundation Model APIs**: Best for getting started, smaller workloads, and standard use cases
- **External Models**: Recommended for large-scale processing, organizational constraints, or when higher concurrency is needed

#### Token Management

LLMs have limits on how much text they can process at once. Switch uses a simple threshold approach:

**How it works:**
- Stage 1 analyzes each SQL file and counts tokens (after removing comments and compressing multiple spaces into a single space)
- Files with token count ≤ `token_count_threshold` are marked for conversion
- Files exceeding the threshold are skipped with status "Not converted"

**Token counting approach:**
- Claude models: Character-based estimation (~3.4 characters per token)
- Other models: Uses tiktoken library with o200k_base encoding
- Rough guideline: 20,000 tokens ≈ 50,000 single-byte characters

**Recommended approach:**
- Default threshold: 20,000 tokens (example value based on stability testing)
- For complex transformations with extended thinking: Consider lower thresholds (e.g., 8,000 tokens)
- Optimal values may vary by model and environment - test in your specific setup for best results

**Managing large files:**
If your SQL files exceed the threshold, consider logical splitting points:
- Separate stored procedures into individual files
- Split by functional modules or business domains
- Maintain referential integrity across split files

#### Performance Optimization

**Concurrency Settings:**
- **Default concurrency**: Set to 4 to accommodate limitations with some Foundation Model API endpoints
- **Scaling for large workloads**: For processing many files simultaneously, consider:
  - **Provisioned Throughput**: Deploy dedicated Foundation Model API capacity with guaranteed throughput
  - **External Models**: Configure external endpoints with higher rate limits
  - **Increased concurrency**: Adjust the `concurrency` parameter based on endpoint capacity

**Monitoring:**
- Watch for rate limiting or throttling responses from model endpoints
- Consider enabling [Inference Tables](https://docs.databricks.com/aws/en/machine-learning/model-serving/inference-tables) to automatically capture requests and responses for detailed monitoring and debugging

---

## Advanced Usage

### Customizable Prompts

Switch supports creating custom conversion prompts for new SQL dialects or specialized conversion requirements.

#### Creating Custom Conversion Prompts

To create a custom conversion prompt:

1. **Create a YAML file** with the required structure
2. **Place it in your Databricks workspace**
3. **Specify the full path** in the `conversion_prompt_yaml` parameter

Custom conversion prompts require two main sections:

##### Required Structure

```yaml
system_message: |
  Convert SQL code to Python code that runs on Databricks according to the following instructions:

  # Input and Output
  - Input: A single SQL file containing one or multiple T-SQL statements
  - Output: Python code with Python comments (in {comment_lang}) explaining the code

  ${common_python_instructions_and_guidelines}

  # Additional Instructions
  1. Convert SQL queries to spark.sql() format
  2. Add clear Python comments explaining the code
  3. Use DataFrame operations instead of loops when possible
  4. Handle errors using try-except blocks

few_shots:
- role: user
  content: |
    SELECT name, age
    FROM users
    WHERE active = 1;
- role: assistant
  content: |
    # Get names and ages of active users
    active_users = spark.sql("""
        SELECT name, age
        FROM users
        WHERE active = 1
    """)
    display(active_users)
```

##### Key Elements

**`system_message` Section**:
- Clear explanation of the conversion purpose
- Definition of input and output formats
- Additional instructions for specific conversions
- Comment language specification (`{comment_lang}` is automatically replaced)
- Reference to common instructions (`${common_python_instructions_and_guidelines}`)

**`few_shots` Section** (Optional but recommended):
- Include examples ranging from simple to complex cases
- Each example demonstrates specific patterns for LLM understanding
- Shows typical conversion patterns for your SQL dialect

#### Best Practices for Custom Prompts

1. **Start with existing YAML files** as templates for your custom dialects
2. **Include specific dialect features** that differ from standard SQL
3. **Provide comprehensive examples** covering edge cases in your SQL dialect
4. **Test thoroughly** with representative SQL files before large-scale usage
5. **Iterate and refine** based on conversion results

#### Reference: Built-in YAML Files

Switch provides built-in YAML configuration files for each of the 8 supported SQL dialects. When creating custom prompts, these built-in configurations serve as excellent starting points - even for supported dialects, customizing the default prompts based on your specific input patterns can significantly improve conversion accuracy.

**Location**: You can find example YAML files in the `pyscripts/conversion_prompt_yaml/` directory within the same workspace folder as the main Switch notebook (`00_main`). These files demonstrate the proper structure and provide dialect-specific examples that you can adapt for your custom requirements.

### Conversion Results and Troubleshooting

#### Understanding Conversion Results

After your Switch job completes, review the conversion results displayed at the end of the `00_main` notebook execution. The results table shows the status of each input file:

- **Successfully converted files**: Ready to use as Databricks notebooks
- **Files requiring attention**: May need manual review or re-processing

If you encounter files that didn't convert successfully, here are the most common issues and their solutions:

#### Files Not Converting (Status: "Not converted")

These files were skipped during the conversion process, typically because they're too large for the model to process effectively.

**Cause**: Input files exceed the token count threshold
**Solutions**:
- Split large input files into smaller, more manageable parts
- Increase the `token_count_threshold` parameter if your LLM model can handle larger inputs
- For models with extended thinking capabilities, consider using lower thresholds with that mode enabled

#### Conversion with Errors (Status: "Converted with errors")

These files were successfully processed by the LLM but the generated code contains syntax errors that need to be addressed.

**Cause**: Files were converted but contain syntax errors
**Solutions**:
- Review syntax error messages at the bottom of output notebooks
- Manually fix errors in the converted notebooks
- Increase `max_fix_attempts` for more automatic error correction attempts
- Verify your model serving endpoint supports the required features

#### Export Failures (Status: "Not exported")

These files were converted successfully but couldn't be exported as notebooks due to size limitations.

**Cause**: Converted content exceeds 10MB size limit
**Solutions**:
- Review and reduce the size of input SQL files
- Split complex procedures into multiple smaller files
- Check for excessive code generation or repetitive patterns
